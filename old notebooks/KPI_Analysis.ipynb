{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4578d1",
   "metadata": {},
   "source": [
    "# KPI and Threshold Criteria Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc719a4",
   "metadata": {},
   "source": [
    "## Import simulation run data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea90752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from model.plot_utils import *\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1872a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'root_dir' directory here should be the local version of 'hydra/hydra_multi_class'\n",
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle ('C:/Users/paruc/Documents/Github/hydra/hydra_multi_class/experiment_block1_experiments20210429.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = root_dir + '/experiment_block1_experiments_sigma2_20210511'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment results files stored in 'experiments' subdirectory\n",
    "paths = sorted(Path(experiment_path).iterdir(), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31af477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the most recent experiment--alternatively, substitute filename for desired experiment results file\n",
    "with open(paths[-1], 'rb') as f:\n",
    "    config_ids, experiments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e82b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative fan plot of experimental results (sanity check desired experiment was loaded)\n",
    "param_fan_plot3(experiments, config_ids, 'a','Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b53cea",
   "metadata": {},
   "source": [
    "## Experiment pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b0fa5",
   "metadata": {},
   "source": [
    "### Define Slippage and Impermanent Loss calculations\n",
    "\n",
    "\n",
    "#### Slippage\n",
    "\n",
    "Slippage is calculated in two different, but related ways: the first is as an elasticity, by measuring the percentage change in the price following a trade with respect to the trade size as a percentage of the pool reserve. The second is as the percentage difference between the effective (actual trade) price and the spot price before the trade, which measures trader expectations on the execution price.\n",
    "\n",
    "#### Impermanent Loss\n",
    "\n",
    "Impermanent Loss (IL) is computed as the difference between the value of an amount of a single asset provided  as liquidity by an LP when exiting the pool, and the value of the intial single asset amount held outside of the pool. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13016b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slippage(assets, rdf, market):\n",
    "    \n",
    "    slippage = []\n",
    "    elasticity = []\n",
    "    res_in = []\n",
    "    res_out = []\n",
    "    trans_in = []\n",
    "    trans_out = []\n",
    "    p_out_for_in = []\n",
    "    \n",
    "    # Assign ids for risk assets IN and OUT for Hydra\n",
    "    asset_in_ids = rdf.asset_random_choice\n",
    "    asset_out_ids = pd.Series([x for y in asset_in_ids for x in assets if x != y  ])\n",
    "    \n",
    "    if market == 'hydra': \n",
    "        \n",
    "        for t in range(0,len(asset_in_ids)-1):\n",
    "            \n",
    "            reserve_asset_in_prev = rdf.pool[t].pool[asset_in_ids[t]]['R']\n",
    "            reserve_asset_in = rdf.pool[t+1].pool[asset_in_ids[t]]['R']\n",
    "            reserve_asset_out = rdf.pool[t+1].pool[asset_out_ids[t]]['R']\n",
    "            reserve_asset_out_prev = rdf.pool[t].pool[asset_out_ids[t]]['R']\n",
    "            price_asset_in = rdf.pool[t+1].pool[asset_in_ids[t]]['P']\n",
    "            price_asset_in_prev = rdf.pool[t].pool[asset_in_ids[t]]['P']\n",
    "            price_asset_out = rdf.pool[t+1].pool[asset_in_ids[t]]['P']\n",
    "            price_asset_out_prev = rdf.pool[t].pool[asset_out_ids[t]]['P']\n",
    "            \n",
    "            transactions_in = rdf.trade_random_size[t]\n",
    "            transactions_out = -(reserve_asset_out - reserve_asset_out_prev)\n",
    "\n",
    "            # If transactions_out or transactions_in is zero, was not a swap/trade event; return n.a.\n",
    "            if transactions_out == 0 or transactions_in == 0:\n",
    "                elasticity.append(np.nan)\n",
    "                slippage.append(np.nan)\n",
    "            else:\n",
    "                # Compute percent change in reserve\n",
    "                reserve_in_pct_change = transactions_in / reserve_asset_in_prev\n",
    "                reserve_out_pct_change = transactions_out / reserve_asset_out_prev\n",
    "\n",
    "                # Compute percent change in price (OUT per IN)\n",
    "                price_out_for_in = price_asset_out / price_asset_in\n",
    "                price_out_for_in_prev = price_asset_out_prev / price_asset_in_prev\n",
    "                price_pct_change = (price_out_for_in - price_out_for_in_prev) / price_out_for_in_prev\n",
    "\n",
    "                # Slippage calculation #1: elasticity of price with respect to transactions size\n",
    "                elasticity.append(price_pct_change / reserve_in_pct_change)\n",
    "                # Slippage calculation #2: percentage difference between effective and spot price\n",
    "\n",
    "                slippage.append(((transactions_in/transactions_out) - price_out_for_in_prev) \n",
    "                                / price_out_for_in_prev)\n",
    "            \n",
    "            res_in.append(reserve_asset_in)\n",
    "            res_out.append(reserve_asset_out)\n",
    "            trans_in.append(transactions_in)\n",
    "            trans_out.append(transactions_out)\n",
    "            p_out_for_in.append(price_out_for_in)\n",
    "\n",
    "        # Extract time series - clunky, refactor into comprehensions for performance\n",
    "        #reserve_asset_in_dict = {}\n",
    "        #reserve_asset_out_dict = {}\n",
    "        #asset_price_in_dict = {}  # Note: in terms of HDX as numeraire\n",
    "        #asset_price_out_dict = {} # Note: in terms of HDX as numeraire\n",
    "        #asset_weight_in_dict = {}\n",
    "        #asset_weight_out_dict = {}\n",
    "        #for t in rdf.pool.keys():\n",
    "        #    reserve_asset_in_dict[t] = rdf.pool[t].pool[asset_in_id]['R']\n",
    "        #    reserve_asset_out_dict[t] = rdf.pool[t].pool[asset_out_id]['R']\n",
    "        #    asset_price_in_dict[t] = rdf.pool[t].pool[asset_in_id]['P']\n",
    "        #    asset_price_out_dict[t] = rdf.pool[t].pool[asset_out_id]['P']\n",
    "        #    asset_weight_in_dict[t] = rdf.pool[t].pool[asset_in_id]['W']\n",
    "        #    asset_weight_out_dict[t] = rdf.pool[t].pool[asset_out_id]['W']\n",
    "        #reserve_asset_in = pd.Series(reserve_asset_in_dict)\n",
    "        #reserve_asset_out = pd.Series(reserve_asset_out_dict)\n",
    "        #asset_price_in = pd.Series(asset_price_in_dict)\n",
    "        #asset_price_out = pd.Series(asset_price_out_dict)\n",
    "        #asset_weight_in = pd.Series(asset_weight_in_dict)\n",
    "        #asset_weight_out = pd.Series(asset_weight_out_dict)\n",
    "        \n",
    "        # Compute price of OUT asset in terms of IN asset\n",
    "        #price_out_for_in = asset_price_out / asset_price_in\n",
    "\n",
    "    elif market == 'uni':\n",
    "        \n",
    "        for t in range(0,len(asset_in_ids)-1):\n",
    "            asset_in_id = 'UNI_' + str(asset_in_ids[t]) + str(asset_out_ids[t])\n",
    "            asset_out_id = 'UNI_' + str(asset_out_ids[t]) + str(asset_in_ids[t])\n",
    "            price_id = 'UNI_P_ij'\n",
    "\n",
    "            reserve_asset_in = rdf[asset_in_id][t+1]\n",
    "            reserve_asset_in_prev = rdf[asset_in_id][t]\n",
    "            reserve_asset_out = rdf[asset_out_id][t+1]\n",
    "            reserve_asset_out_prev = rdf[asset_out_id][t]\n",
    "            \n",
    "            transactions_in = rdf.trade_random_size[t]\n",
    "            transactions_out = -(reserve_asset_out - reserve_asset_out_prev)\n",
    "\n",
    "            # If transactions_out or transactions_in is zero, was not a swap/trade event; return n.a.\n",
    "            if transactions_out == 0 or transactions_in == 0:\n",
    "                elasticity.append(np.nan)\n",
    "                slippage.append(np.nan)\n",
    "            else:\n",
    "                # Compute percent change in reserve\n",
    "                reserve_in_pct_change = transactions_in / reserve_asset_in_prev\n",
    "                reserve_out_pct_change = transactions_out / reserve_asset_out_prev\n",
    "                # Compute percent change in price (OUT per IN)\n",
    "                if asset_in_ids[t] == 'i':\n",
    "                    price_out_for_in = rdf[price_id][t+1]\n",
    "                    price_out_for_in_prev = rdf[price_id][t]\n",
    "                elif asset_in_ids[t] == 'j':\n",
    "                    price_out_for_in = 1 / rdf[price_id][t+1]\n",
    "                    price_out_for_in_prev = 1 / rdf[price_id][t]\n",
    "                price_pct_change = (price_out_for_in - price_out_for_in_prev) / price_out_for_in_prev\n",
    "\n",
    "                # Slippage calculation #1: elasticity of price with respect to transactions size\n",
    "                elasticity.append(price_pct_change / reserve_in_pct_change)\n",
    "\n",
    "                # Slippage calculation #2: percentage difference between effective and spot price\n",
    "                slippage.append(((transactions_in/transactions_out) - price_out_for_in_prev) \n",
    "                                / price_out_for_in_prev)\n",
    "            \n",
    "            res_in.append(reserve_asset_in)\n",
    "            res_out.append(reserve_asset_out)\n",
    "            trans_in.append(transactions_in)\n",
    "            trans_out.append(transactions_out)\n",
    "            p_out_for_in.append(price_out_for_in)\n",
    "        \n",
    "        #asset_in_id = 'UNI_R' + str(asset_in)\n",
    "        #asset_out_id = 'UNI_R' + str(asset_out)\n",
    "        #asset_in_swap_id = 'UNI_' + str(asset_in) + str(asset_out)\n",
    "        #asset_out_swap_id = 'UNI_' + str(asset_out) + str(asset_in)\n",
    "        #asset_price_in = 'UNI_P_RQ' + str(asset_in) # Note: in terms of HDX as numeraire\n",
    "        #asset_price_out = 'UNI_P_RQ' + str(asset_out) # Note: in terms of HDX as numeraire\n",
    "        #asset_price_swap = 'UNI_P_ij'\n",
    "        \n",
    "        #reserve_asset_in = rdf[asset_in_id]\n",
    "        #reserve_asset_out = rdf[asset_out_id]\n",
    "        #asset_price_in = rdf[asset_price_in]\n",
    "        #asset_price_out = rdf[asset_price_out]\n",
    "        #reserve_asset_in = rdf[asset_in_swap_id]\n",
    "        #reserve_asset_out = rdf[asset_out_swap_id]\n",
    "        \n",
    "        #if asset_in == 'i':\n",
    "        #    price_out_for_in = rdf[asset_price_swap]\n",
    "        #elif asset_in == 'j':\n",
    "        #    price_out_for_in = 1 / rdf[asset_price_swap]\n",
    "        \n",
    "        # 8 May 2021: column 'UNI_Rx' keeps track of Q <-> asset and add/remove Liquidity\n",
    "        # columns 'UNI_ij' & 'UNI_ji' keep track of asset i <-> asset j swap/trades\n",
    "        # To get full reserve balance effects, need to diff and update 'UNI_Rx' columns\n",
    "        # reserve_asset_in = pd.Series([rdf[asset_in_id][0]])\n",
    "        # reserve_asset_out = pd.Series([rdf[asset_out_id][0]])\n",
    "        #for t in range(1,len(rdf[asset_in_id].diff())):\n",
    "        #    add_in = pd.Series([reserve_asset_in[t-1] + rdf[asset_in_id].diff()[t] + \n",
    "        #                           rdf[asset_in_swap_id].diff()[t]])\n",
    "        #    add_out = pd.Series([reserve_asset_out[t-1] + rdf[asset_out_id].diff()[t] + \n",
    "        #                           rdf[asset_out_swap_id].diff()[t]])\n",
    "        #    reserve_asset_in = reserve_asset_in.append(add_in, ignore_index = True)\n",
    "        #    reserve_asset_out = reserve_asset_out.append(add_out, ignore_index = True)\n",
    "    \n",
    "    return {\n",
    "        'reserve_asset_in' : pd.Series(res_in),\n",
    "        'reserve_asset_out' : pd.Series(res_out),\n",
    "        'transactions_in' : pd.Series(trans_in), \n",
    "        'transactions_out' : pd.Series(trans_out),\n",
    "        'price_out_for_in' : pd.Series(p_out_for_in),\n",
    "        'elasticity' : pd.Series(elasticity), \n",
    "        'slippage' : pd.Series(slippage)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18693e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impermanent_loss(agent_id, asset_id, liquidity_added, time_entered, time_exited, rdf, market):\n",
    "    \n",
    "    # Set ids for asset that the LP has added\n",
    "    agent_asset_id = 'r_' + str(asset_id) + '_out'\n",
    "    share_id = 's_' + str(asset_id)\n",
    "\n",
    "    # Set amount added by LP during addLiquidity event \n",
    "    # (usually set as an initial condition of the simulation)\n",
    "    liquidity_added = liquidity_added\n",
    "\n",
    "    # Set timestamp of addLiquidity event for LP\n",
    "    time_liquidity_in = time_entered # Note: this is different for Uniswap agent than Hydra agent LP!\n",
    "\n",
    "    # Set timestamp of removeLiquidity event for LP\n",
    "    time_liquidity_out = time_exited\n",
    "    \n",
    "    # Build cumulative transactions IN amount (assumes fee eventually assessed to IN asset)\n",
    "    transactions_in_cumulative = rdf.trade_random_size.cumsum()[:time_liquidity_out]\n",
    "    transactions_in_cumulative.name = \"transactions_in_cumulative\"\n",
    "\n",
    "    if market == 'hydra':\n",
    "        \n",
    "        # Get share awarded to LP from addLiquidity event\n",
    "        share_rewarded = ( rdf['hydra_agents'][time_liquidity_in].iloc[agent_id][share_id] - \n",
    "                             rdf['hydra_agents'][time_liquidity_in - 1].iloc[agent_id][share_id] )\n",
    "\n",
    "        # Compute impermanent loss over entire time series\n",
    "        Ri_over_Wi = []\n",
    "        Price = []\n",
    "        # Build loop computing IL for every timestep\n",
    "        for t in rdf['hydra_agents'].keys():\n",
    "            if t <= time_liquidity_out:\n",
    "                Ri_over_Wi.append(rdf.pool[t].pool[asset_id]['R'] / rdf.pool[t].pool[asset_id]['W'])\n",
    "                Price.append(rdf.pool[t].pool[asset_id]['P'])\n",
    "        Price = pd.Series(Price)\n",
    "        Ri_over_Wi = pd.Series(Ri_over_Wi)\n",
    "        Wq_over_Sq = rdf['Wq'][:time_liquidity_out] / rdf['Sq'][:time_liquidity_out]\n",
    "        liquidity_removed = Wq_over_Sq.reset_index(drop=True) * Ri_over_Wi.reset_index(drop = True) * share_rewarded\n",
    "        hold_value = liquidity_added * Price\n",
    "        pool_value = liquidity_removed * Price\n",
    "        \n",
    "        # Get reserve time series for asset the LP has added\n",
    "        reserve_asset_in = []\n",
    "        for t in rdf.pool.keys():\n",
    "            if t<= time_liquidity_out:\n",
    "                reserve_asset_in.append(rdf.pool[t].pool[str(asset_id)]['R'])\n",
    "        reserve_asset_in = pd.Series(reserve_asset_in, name='reserve_asset_in')\n",
    "        \n",
    "    elif market == 'uni':\n",
    "        \n",
    "        asset_reserve_key = 'UNI_R' + str(asset_id)\n",
    "        asset_share_key = 'UNI_S' + str(asset_id)\n",
    "        asset_price_key = 'UNI_P_RQ' + str(asset_id)\n",
    "        asset_in_id = 'UNI_R' + str(asset_id)\n",
    "        if asset_id == 'i':\n",
    "            asset_out_id = 'j'\n",
    "        elif asset_id == 'j':\n",
    "            asset_out_id = 'i'\n",
    "        asset_in_swap_id = 'UNI_' + str(asset_id) + str(asset_out_id)\n",
    "\n",
    "        # Get share awarded to LP from addLiquidity event\n",
    "        share_rewarded = ( rdf['uni_agents'][time_liquidity_in].iloc[agent_id][share_id] - \n",
    "                             rdf['uni_agents'][time_liquidity_in - 1].iloc[agent_id][share_id] )\n",
    "\n",
    "        # Compute impermanent loss over entire time series\n",
    "\n",
    "        liquidity_removed = (rdf[asset_reserve_key][:time_liquidity_out+1] / \n",
    "                               rdf[asset_share_key][:time_liquidity_out+1]) * share_rewarded\n",
    "        pool_value = liquidity_removed * rdf[asset_price_key][:time_liquidity_out+1] \n",
    "        hold_value = liquidity_added * rdf[asset_price_key][:time_liquidity_out+1]\n",
    "        \n",
    "        # Get reserve time series for asset the LP has added\n",
    "        # 8 May 2021: column 'UNI_Rx' keeps track of Q <-> asset and add/remove Liquidity\n",
    "        # columns 'UNI_ij' & 'UNI_ji' keep track of asset i <-> asset j swap/trades\n",
    "        # To get full reserve balance effects, need to diff and update 'UNI_Rx' columns\n",
    "        #reserve_asset_out = pd.Series([rdf[asset_out_id][0]])\n",
    "        reserve_asset_in = [rdf[asset_in_id][0]]\n",
    "        for t in range(1,len(rdf[asset_in_id].diff())):\n",
    "            if t <= time_liquidity_out:\n",
    "                add_in = reserve_asset_in[t-1] + rdf[asset_in_id].diff()[t] + rdf[asset_in_swap_id].diff()[t]\n",
    "            #add_out = pd.Series([reserve_asset_out[t-1] + rdf[asset_out_id].diff()[t] + \n",
    "            #                       rdf[asset_out_swap_id].diff()[t]])\n",
    "                reserve_asset_in.append(add_in)\n",
    "        reserve_asset_in = pd.Series(reserve_asset_in, name='reserve_asset_in')\n",
    "            #reserve_asset_out = reserve_asset_out.append(add_out, ignore_index = True)\n",
    "    \n",
    "    IL = (pool_value / hold_value) - 1\n",
    "    return {\n",
    "        'pool_value' : pool_value,\n",
    "        'hold_value' : hold_value,\n",
    "        'impermanent_loss' : IL,\n",
    "        'liquidity_removed' : liquidity_removed,\n",
    "        'transactions_in_cumulative' : transactions_in_cumulative,\n",
    "        'reserve_asset_in' : reserve_asset_in\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bf865",
   "metadata": {},
   "source": [
    "#### For debugging only: create one representative 'rdf' result DataFrame for subset 0 and simulation 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_array = experiments['subset'].unique()\n",
    "MC_simulation_array = experiments['simulation'].unique()\n",
    "experiment_by_subset = experiments.sort_values(by=['subset']).reset_index(drop=True)\n",
    "sub_ex = experiment_by_subset[experiment_by_subset['subset']==0].copy()\n",
    "rdf = sub_ex[sub_ex['simulation']==0].sort_values(by=['timestep']).reset_index(drop=True).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff04888",
   "metadata": {},
   "source": [
    "## Compute Slippage KPI time series across subsets and simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebfe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_kpis = {}\n",
    "assets = ['i', 'j']\n",
    "for subset in subset_array:\n",
    "    kpi_subset = { 'Hydra' : {}, 'UNI' : {} }\n",
    "    sub_experiments = experiment_by_subset[experiment_by_subset['subset']==subset].copy()\n",
    "    for simulation in MC_simulation_array:\n",
    "        sub_monte_carlo = sub_experiments[sub_experiments['simulation'] == simulation]\n",
    "        rdf = sub_monte_carlo.sort_values(by=['timestep']).reset_index(drop=True).copy()\n",
    "        # the following snippet is for future reference if fees are analyzed\n",
    "        # requires that the 'config_ids' variable be saved from experiments\n",
    "        # *****\n",
    "        #config_rdf = [x for x in config_ids if x['simulation_id'] == simulation and \n",
    "        #              x['subset_id'] == subset][0]\n",
    "        #fee = 1 - config_rdf['M']['fee_numerator']/config_rdf['M']['fee_denominator']\n",
    "        # *****\n",
    "        print(\"***Slippage calc for sim \", simulation, \" of subset \", subset, \"***\")\n",
    "        kpi_subset['Hydra'].update({simulation : slippage(assets, rdf, 'hydra')})\n",
    "        kpi_subset['UNI'].update({simulation : slippage(assets, rdf, 'uni')})\n",
    "    sl_kpis.update({subset: kpi_subset})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca8dc5",
   "metadata": {},
   "source": [
    "## Compute Impermanent Loss KPI time series across subsets and simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_kpis = {}\n",
    "\n",
    "# The following information should ideally be read in from e.g. 'config_ids',\n",
    "# so that minimal intervention is required. Here, these are manually entered for\n",
    "# the experiment being analyzed.\n",
    "# **********\n",
    "asset_in = 'i'\n",
    "lp_agent_number = 2\n",
    "liquidity_added = 50000\n",
    "time_entered_hydra = 10\n",
    "time_entered_uni = 10\n",
    "time_exited = 90\n",
    "# **********\n",
    "\n",
    "for subset in subset_array:\n",
    "    kpi_subset = { 'Hydra' : {}, 'UNI' : {} }\n",
    "    sub_experiments = experiment_by_subset[experiment_by_subset['subset']==subset].copy()\n",
    "    for simulation in MC_simulation_array:\n",
    "        sub_monte_carlo = sub_experiments[sub_experiments['simulation'] == simulation]\n",
    "        rdf = sub_monte_carlo.sort_values(by=['timestep']).reset_index(drop=True).copy()\n",
    "        print(\"***IL calc for sim \", simulation, \" of subset \", subset, \"***\")\n",
    "        kpi_subset['Hydra'].update({simulation : impermanent_loss(\n",
    "            lp_agent_number, asset_in, liquidity_added, \n",
    "            time_entered_hydra, time_exited, \n",
    "            rdf, 'hydra')})\n",
    "        kpi_subset['UNI'].update({simulation : impermanent_loss(\n",
    "            lp_agent_number, asset_in, liquidity_added, \n",
    "            time_entered_uni, time_exited, \n",
    "            rdf, 'uni')})\n",
    "    il_kpis.update({subset: kpi_subset})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe305d",
   "metadata": {},
   "source": [
    "## (In progress) Plot KPI time series as fan plots, one per sweep value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in subset_array:\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(15,7))\n",
    "    ax1.plot([sl_kpis[subset]['Hydra'][x]['slippage'] for x in sl_kpis[subset]['Hydra']])\n",
    "    ax2.plot([sl_kpis[subset]['UNI'][x]['slippage'] for x in sl_kpis[subset]['Hydra']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243abe9",
   "metadata": {},
   "source": [
    "## Compute Threshold KPIs (regression results) from KPI time series\n",
    "(Note that printing of regression results can be enabled/disabled using the PRINT constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT = True\n",
    "\n",
    "coeffs = {\n",
    "    'slippage' : [],\n",
    "    'elasticity' : [],\n",
    "    'impermanent_loss' : []\n",
    "}\n",
    "\n",
    "kpi_threshold_values = {\n",
    "    'Hydra' : coeffs.copy(),\n",
    "    'UNI' : coeffs.copy()\n",
    "}\n",
    "\n",
    "kpi_thresholds = {}\n",
    "\n",
    "for subset in subset_array:\n",
    "    kpi_threshold_values = {\n",
    "        'Hydra' : coeffs.copy(),\n",
    "        'UNI' : coeffs.copy()\n",
    "    }\n",
    "    for simulation in MC_simulation_array:\n",
    "        for market in ['Hydra', 'UNI']:\n",
    "            for measure in ['elasticity', 'slippage', 'impermanent_loss']: \n",
    "            #for measure in ['impermanent_loss']: \n",
    "                if measure in ['elasticity']:\n",
    "                    depvar = sl_kpis[subset][market][simulation][measure]\n",
    "                    indepvar = sl_kpis[subset][market][simulation]['transactions_out']\n",
    "                    xname = ['Transactions Size']\n",
    "                elif measure in ['slippage']:\n",
    "                    depvar = sl_kpis[subset][market][simulation][measure]\n",
    "                    indepvar = sl_kpis[subset][market][simulation]['transactions_out']\n",
    "                    indepvar = sm.add_constant(indepvar)\n",
    "                    xname = ['Constant', 'Transactions Size']\n",
    "                elif measure in ['impermanent_loss']:\n",
    "                    depvar = il_kpis[subset][market][simulation][measure]\n",
    "                    indepvar = pd.concat([il_kpis[subset][market][simulation]['transactions_in_cumulative'],\n",
    "                                        il_kpis[subset][market][simulation]['reserve_asset_in']], axis=1)\n",
    "                    indepvar = sm.add_constant(indepvar)\n",
    "                    xname = ['Constant', 'Transactions Size', 'Balance Size']\n",
    "                if PRINT:\n",
    "                    print('********************')\n",
    "                    print('Market: ', market, '; measure: ', measure)\n",
    "                    print('********************')\n",
    "        \n",
    "                try:\n",
    "                    model = sm.OLS(depvar,indepvar, missing = 'drop')\n",
    "                    results = model.fit()\n",
    "                    if PRINT:\n",
    "                        print(results.summary(xname=xname))\n",
    "                    kpi_threshold_values[market][measure].append((results.params, results.pvalues))\n",
    "                except ValueError as e:\n",
    "                    print('Market: ', market, ' with measure: ', measure, '; error: ', e)\n",
    "    kpi_thresholds.update({subset : kpi_threshold_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a1f3b",
   "metadata": {},
   "source": [
    "## KPI Threshold Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccc9ab",
   "metadata": {},
   "source": [
    "Recall threshold criteria (as of 7 May 2021) are:\n",
    "1. The estimated coefficients of the Hydra elasticity regressions should be less than one more than 80% of the MC runs.\n",
    "2. The estimated coefficients of the Hydra slippage regressions should be less than the associated coefficient values of the Uniswap slippage regressions more than 80% of the MC runs, when transactions are the same across both markets.\n",
    "3. The estimated constant and trade/swap transactions size coefficients of the Hydra impermanent loss regressions should be less than the associated coefficient values of the Uniswap impermanent loss regressions more than 80% of the MC runs, when transactions are the same across both markets.\n",
    "4. The estimated reserve balance coefficient of the Hydra impermanent loss regressions should be statistically no different from zero for at least 80% of the MC runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd745e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TC1\n",
    "TC1 = {}\n",
    "for subset in subset_array:\n",
    "    mar = {}\n",
    "    kpi_threshold_values = kpi_thresholds[subset]\n",
    "    for market in ['Hydra', 'UNI']:\n",
    "        elasticity_coeffs = kpi_threshold_values[market]['elasticity']\n",
    "        satisfy = len([x for x in elasticity_coeffs[0] if x.item() < 1])\n",
    "        fraction = satisfy/len(elasticity_coeffs)\n",
    "        mar.update({market : fraction})\n",
    "    TC1.update({ subset : mar })\n",
    "print(\"TC1: \", TC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TC2\n",
    "TC2 = {}\n",
    "for subset in subset_array:\n",
    "    kpi_threshold_values = kpi_thresholds[subset]\n",
    "    slippage_coeffs_h = kpi_threshold_values['Hydra']['slippage']\n",
    "    slippage_coeffs_u = kpi_threshold_values['UNI']['slippage']\n",
    "    satisfy = len([x for i, x in enumerate(slippage_coeffs_h[0]) if\n",
    "                          x['const'] < slippage_coeffs_u[0][i]['const'] and\n",
    "                          x[0] < slippage_coeffs_u[0][i][0]])\n",
    "    fraction = satisfy/len(slippage_coeffs_h)\n",
    "    TC2.update({subset: fraction})\n",
    "print(\"TC2: \", TC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TC3, TC4\n",
    "TC3 = {}; TC4 = {}\n",
    "two_sided_significance = 0.05\n",
    "for subset in subset_array:\n",
    "    kpi_threshold_values = kpi_thresholds[subset]\n",
    "    il_coeffs_h = kpi_threshold_values['Hydra']['impermanent_loss']\n",
    "    il_coeffs_u = kpi_threshold_values['UNI']['impermanent_loss']\n",
    "    satisfyTC3 = len([x for i, x in enumerate(il_coeffs_h[0]) if\n",
    "                          x['const'] < il_coeffs_u[0][i]['const'] and\n",
    "                          x['transactions_in_cumulative'] < il_coeffs_u[0][i]['transactions_in_cumulative']])\n",
    "    satisfyTC4 = len([x for i, x in enumerate(il_coeffs_h[1]) if\n",
    "                          x['reserve_asset_in'] < two_sided_significance])\n",
    "    fractionTC3 = satisfyTC3/len(il_coeffs_h)\n",
    "    fractionTC4 = satisfyTC4/len(il_coeffs_h)\n",
    "    TC3.update({subset: fractionTC3})\n",
    "    TC4.update({subset: fractionTC4})\n",
    "print(\"TC3: \", TC3)\n",
    "print(\"TC4: \", TC4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a921fd",
   "metadata": {},
   "source": [
    "## Possible future refinement: remove outliers from regresssion results using outlier test\n",
    "Outlier test from: [StackOverflow](https://stackoverflow.com/questions/11882393/matplotlib-disregard-outliers-when-plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(points, thresh=3.5):\n",
    "    \"\"\"\n",
    "    Returns a boolean array with True if points are outliers and False \n",
    "    otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        points : An numobservations by numdimensions array of observations\n",
    "        thresh : The modified z-score to use as a threshold. Observations with\n",
    "            a modified z-score (based on the median absolute deviation) greater\n",
    "            than this value will be classified as outliers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        mask : A numobservations-length boolean array.\n",
    "\n",
    "    References:\n",
    "    ----------\n",
    "        Boris Iglewicz and David Hoaglin (1993), \"Volume 16: How to Detect and\n",
    "        Handle Outliers\", The ASQC Basic References in Quality Control:\n",
    "        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. \n",
    "    \"\"\"\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
